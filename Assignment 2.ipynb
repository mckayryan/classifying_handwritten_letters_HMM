{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COMP9418 Assignment 2\n",
    "\n",
    "## *Tasks TODO*\n",
    "- parameter initialization\n",
    "- baseline (GMM model)\n",
    "- mean negative log probability\n",
    "- sample predictions.txt generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from scipy.misc import logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hmmlearn\n",
    "import sklearn\n",
    "from hmmlearn.hmm import GaussianHMM, GMMHMM\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure it's version 0.2.1 (install from github, not pip)\n",
    "# pip install git+https://github.com/hmmlearn/hmmlearn.git\n",
    "hmmlearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.0'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "trainData = sio.loadmat('./trajectories_train.mat')\n",
    "testData = sio.loadmat('./trajectories_xtest.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean up the data\n",
    "xtrain = trainData['xtrain'].reshape((-1, ))\n",
    "ytrain = trainData['ytrain'].reshape((-1, ))\n",
    "kf = StratifiedKFold(n_splits = 3, random_state=rng)\n",
    "xtest = testData['xtest'].reshape((-1, ))\n",
    "key = trainData['key']\n",
    "key = [item[0] for item in key.reshape((-1, ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1429L,)\n",
      "(1429L,)\n",
      "(1429L,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtest.shape)\n",
    "# print([sum(yval == i) for i in np.unique(ytrain)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idx = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = xtrain[idx]\n",
    "y = ytrain[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_char(data, label):\n",
    "    start_x = 0\n",
    "    start_y = 0\n",
    "    plt.plot(start_x, start_y, 'ro')\n",
    "    for vel_h, vel_v, alpha in zip(data[0,], data[1, ], 1/(1 + np.exp(-data[2, ]/np.sum(data[2, ])))):\n",
    "        start_x = start_x + vel_h\n",
    "        start_y = start_y + vel_v\n",
    "        plt.plot(start_x, start_y,'bo', alpha = alpha)\n",
    "    plt.title('Character ' + key[label-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+CIiNUIBX0SkRijgi4jUCAV8EZEa8f8BG7br6Bt+XG4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4c04048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_char(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xtrain = np.asarray([seq.T for seq in xtrain])\n",
    "xtest = np.asarray([seq.T for seq in xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_max = np.max(np.vstack(xtrain), axis=0)\n",
    "train_min = np.min(np.vstack(xtrain), axis=0)\n",
    "def rescale(seq):\n",
    "    return (seq - train_min) / (train_max - train_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xtrain = np.asarray([rescale(seq) for seq in xtrain])\n",
    "xtest = np.asarray([rescale(seq) for seq in xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1429L,)\n",
      "(1429L,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lengths_train = list(map(lambda x: x.shape[0], xtrain))\n",
    "lengths_test = list(map(lambda x: x.shape[0], xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20], dtype=uint8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc = LabelEncoder().fit(ytrain)\n",
    "label_enc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generative Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(hmm, sequence):\n",
    "\n",
    "    logprob_frame = hmm._compute_log_likelihood(sequence)\n",
    "    logprob_sequence, _ =  hmm._do_forward_pass(logprob_frame)\n",
    "\n",
    "    return logprob_sequence\n",
    "\n",
    "def log_likelihoods(hmm, sequences):\n",
    "\n",
    "    ll = lambda seq: log_likelihood(hmm, seq)\n",
    "\n",
    "    return np.fromiter(map(ll, sequences), dtype='float64')\n",
    "\n",
    "def log_likelihoods_cond(cond_hmms, sequences):\n",
    "\n",
    "    ll = lambda hmm: log_likelihoods(hmm, sequences)\n",
    "\n",
    "    return np.vstack(map(ll, cond_hmms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make sure to keep the init_params commented otherwise it doesn't work properly for some reason\n",
    "class GenerativeClassifierHMM(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hmm=GaussianHMM()):\n",
    "        self.hmm = hmm\n",
    "        self.class_cond_hmms_ = []\n",
    "\n",
    "    def fit(self, sequences, labels, k):        \n",
    "        class_counts = np.bincount(labels).astype(np.float)\n",
    "        self.logprior = np.log(class_counts / np.sum(class_counts))\n",
    "        \n",
    "        for c in range(np.max(labels)+1):\n",
    "            sequences_c = sequences[labels == c]\n",
    "            X_c = np.vstack(sequences_c)\n",
    "            lengths_c = list(map(len, sequences_c))\n",
    "            class_cond_hmm = clone(self.hmm, safe=True)\n",
    "            n_states_k = k[c]\n",
    "            pi0 = np.eye(1, n_states_k)[0]\n",
    "            trans0 = np.diag(np.ones(n_states_k)) + np.diag(np.ones(n_states_k-1), 1)\n",
    "            trans0 /= trans0.sum(axis=1).reshape(-1, 1)\n",
    "            class_cond_hmm.n_components = n_states_k\n",
    "            class_cond_hmm.startprob_ = pi0\n",
    "            class_cond_hmm.transmat_  = trans0\n",
    "            class_cond_hmm.fit(X_c, lengths=lengths_c)\n",
    "            self.class_cond_hmms_.append(class_cond_hmm)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, sequences):\n",
    "        # 20 x N matrix\n",
    "        log_likelihood_ = log_likelihoods_cond(self.class_cond_hmms_, sequences)\n",
    "\n",
    "        log_post_unnorm = log_likelihood_ + self.logprior.reshape(-1, 1)\n",
    "\n",
    "        return np.argmax(log_post_unnorm, axis=0)\n",
    "    \n",
    "    def predict_proba(self, sequences):\n",
    "        log_likelihood_ = log_likelihoods_cond(self.class_cond_hmms_, sequences)\n",
    "\n",
    "        log_post_unnorm = log_likelihood_ + self.logprior.reshape(-1, 1)\n",
    "        \n",
    "        prob_post_norm = np.empty_like(log_post_unnorm)\n",
    "        \n",
    "        for i in range(log_post_unnorm.shape[1]):\n",
    "            prob_post_norm[:,i] = log_post_unnorm[:,i] - logsumexp(log_post_unnorm[:,i].astype(np.float64))\n",
    "            \n",
    "        return prob_post_norm\n",
    "    \n",
    "    def generateSample(self, mClass, length):\n",
    "        sel_hmm = self.class_cond_hmms_[mClass]\n",
    "        x, _ = sel_hmm.sample(length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Guassian HMM (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# due to https://github.com/hmmlearn/hmmlearn/issues/158 and \n",
    "# https://github.com/hmmlearn/hmmlearn/issues/175\n",
    "# the fitting process is going to give a LOT of warnings\n",
    "# so we hide them in this notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-f8b8c3bcd765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m hmm_classifier.fit(xtrain[train_index], \n\u001b[0;32m      7\u001b[0m                    \u001b[0mlabel_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                    np.tile(10, 20))\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-57c893230833>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, sequences, labels, k)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mclass_cond_hmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartprob_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mclass_cond_hmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransmat_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtrans0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mclass_cond_hmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlengths_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_cond_hmms_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_cond_hmm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\hmmlearn\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, lengths)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \"\"\"\n\u001b[0;32m    423\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\hmmlearn\\hmm.pyc\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, X, lengths)\u001b[0m\n\u001b[0;32m    195\u001b[0m             kmeans = cluster.KMeans(n_clusters=self.n_components,\n\u001b[0;32m    196\u001b[0m                                     random_state=self.random_state)\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'c'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_params\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"covars_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cluster\\k_means_.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    891\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    894\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cluster\\k_means_.pyc\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cluster\\k_means_.pyc\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;31m# init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n\u001b[1;32m--> 395\u001b[1;33m                               x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[0;32m    396\u001b[0m     \u001b[0mcenters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cluster\\k_means_.pyc\u001b[0m in \u001b[0;36m_init_centroids\u001b[1;34m(X, k, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         centers = _k_init(X, k, random_state=random_state,\n\u001b[1;32m--> 684\u001b[1;33m                           x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[0;32m    685\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'random'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\cluster\\k_means_.pyc\u001b[0m in \u001b[0;36m_k_init\u001b[1;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m# to the squared distance to the closest existing center\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mrand_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_local_trials\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcurrent_pot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),\n\u001b[0m\u001b[0;32m    109\u001b[0m                                         rand_vals)\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\sklearn\\utils\\extmath.pyc\u001b[0m in \u001b[0;36mstable_cumsum\u001b[1;34m(arr, axis, rtol, atol)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mexpected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol,\n\u001b[1;32m--> 763\u001b[1;33m                              atol=atol, equal_nan=True)):\n\u001b[0m\u001b[0;32m    764\u001b[0m         warnings.warn('cumsum was found to be unstable: '\n\u001b[0;32m    765\u001b[0m                       \u001b[1;34m'its last element does not correspond to sum'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\numpy\\core\\numeric.pyc\u001b[0m in \u001b[0;36misclose\u001b[1;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[0;32m   2451\u001b[0m     \u001b[0myfin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2452\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2453\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2454\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2455\u001b[0m         \u001b[0mfinite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxfin\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0myfin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\numpy\\core\\numeric.pyc\u001b[0m in \u001b[0;36mwithin_tol\u001b[1;34m(x, y, atol, rtol)\u001b[0m\n\u001b[0;32m   2433\u001b[0m     \"\"\"\n\u001b[0;32m   2434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2435\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2436\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mless_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\numpy\\core\\numeric.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2967\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2968\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moldstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseterr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2969\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moldcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseterrcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\numpy\\core\\numeric.pyc\u001b[0m in \u001b[0;36mseterr\u001b[1;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[0;32m   2653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2654\u001b[0m     \u001b[0mpyvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeterrobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2655\u001b[1;33m     \u001b[0mold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeterr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdivide\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\matty\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\numpy\\core\\numeric.pyc\u001b[0m in \u001b[0;36mgeterr\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2717\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'divide'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_errdict_rev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2718\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmaskvalue\u001b[0m \u001b[1;33m>>\u001b[0m \u001b[0mSHIFT_OVERFLOW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2719\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'over'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_errdict_rev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2720\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmaskvalue\u001b[0m \u001b[1;33m>>\u001b[0m \u001b[0mSHIFT_UNDERFLOW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2721\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'under'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_errdict_rev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm = GaussianHMM(n_components=25, n_iter=10, random_state=seed)\n",
    "hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "train_index, test_index = list(kf.split(xtrain, ytrain))[0]\n",
    "\n",
    "hmm_classifier.fit(xtrain[train_index], \n",
    "                   label_enc.transform(ytrain[train_index]),\n",
    "                   np.tile(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Validation Accuracy', (y_val_pred == ytrain[test_index]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain[train_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Training Accuracy', (y_train_pred == ytrain[train_index]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs = hmm_classifier.predict_proba(xtest)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n"
     ]
    }
   ],
   "source": [
    "print(key[np.argmax(probs[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1063c748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 53\n",
    "x = xtest[index] * (train_max - train_min) + train_min\n",
    "plot_char(x.T, np.argmax(probs[:,index])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "g\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(np.round(probs[:,idx], 4))\n",
    "print(key[np.argmax(probs[:,idx]) + 1])\n",
    "print(key[ytrain[train_index][idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   b   c   d   e   g   h   l   m   n   o   p   q   r   s   u   v   w   y  \\\n",
       "a  28   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "b   0  26   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   1   0   \n",
       "c   0   0  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "d   0   0   0  24   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "e   0   0   0   0  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "g   0   0   0   0   0  25   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "h   0   0   0   0   0   0  10   0   1   4   0   0   0   0   0   0   0   4   0   \n",
       "l   0   0   0   0   0   0   0  25   0   0   0   0   0   0   0   0   0   2   0   \n",
       "m   0   0   0   0   0   0   0   0  18   3   0   0   0   0   0   0   0   2   0   \n",
       "n   0   0   0   0   0   0   0   0   4  16   0   0   0   0   0   0   0   1   0   \n",
       "o   0   0   0   0   0   0   0   0   0   0  22   0   0   0   0   0   0   0   0   \n",
       "p   0   0   0   0   0   0   0   0   0   0   0  24   0   0   0   0   0   0   0   \n",
       "q   0   0   0   0   0   0   0   0   0   1   0   0  18   0   0   0   0   0   0   \n",
       "r   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   \n",
       "s   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22   0   0   0   0   \n",
       "u   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  18   0   4   0   \n",
       "v   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  30   0   0   \n",
       "w   0   0   0   0   0   0   0   0   2   1   0   0   0   0   0   1   0  16   0   \n",
       "y   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  23   \n",
       "z   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "    z  \n",
       "a   0  \n",
       "b   0  \n",
       "c   0  \n",
       "d   0  \n",
       "e   0  \n",
       "g   0  \n",
       "h   0  \n",
       "l   0  \n",
       "m   0  \n",
       "n   0  \n",
       "o   0  \n",
       "p   0  \n",
       "q   0  \n",
       "r   0  \n",
       "s   0  \n",
       "u   0  \n",
       "v   0  \n",
       "w   0  \n",
       "y   0  \n",
       "z  31  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "conf_mat = pandas.DataFrame(confusion_matrix(ytrain[test_index], y_val_pred))\n",
    "conf_mat.columns = key\n",
    "conf_mat.index = key\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   b   c   d   e   g   h   l   m   n   o   p   q   r   s   u   v   w   y  \\\n",
       "a  55   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "b   0  55   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   \n",
       "c   0   0  44   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "d   0   0   0  47   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "e   0   0   0   0  64   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "g   0   0   0   0   0  50   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "h   0   0   0   0   0   0  24   0   1   7   0   0   0   0   0   0   0   6   0   \n",
       "l   0   0   0   0   0   0   3  48   0   0   0   0   0   0   0   0   0   1   0   \n",
       "m   0   0   0   0   0   0   2   0  37   2   0   0   0   0   0   0   0   3   0   \n",
       "n   0   0   0   0   0   0   0   0   3  35   0   0   0   0   0   0   0   3   0   \n",
       "o   0   0   0   0   0   0   0   0   0   0  44   0   0   0   0   0   0   0   0   \n",
       "p   0   0   0   0   0   0   0   0   0   0   0  46   0   0   0   0   0   0   0   \n",
       "q   0   0   0   0   0   0   0   0   0   0   0   0  38   0   0   0   0   0   0   \n",
       "r   0   0   0   0   0   0   0   0   0   0   0   0   0  38   0   0   0   0   0   \n",
       "s   0   0   0   0   0   0   0   0   0   0   0   0   0   0  43   0   0   0   0   \n",
       "u   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  39   0   3   0   \n",
       "v   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  60   0   0   \n",
       "w   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0  36   0   \n",
       "y   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  45   \n",
       "z   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "    z  \n",
       "a   0  \n",
       "b   0  \n",
       "c   0  \n",
       "d   0  \n",
       "e   0  \n",
       "g   0  \n",
       "h   0  \n",
       "l   0  \n",
       "m   0  \n",
       "n   0  \n",
       "o   0  \n",
       "p   0  \n",
       "q   0  \n",
       "r   0  \n",
       "s   0  \n",
       "u   0  \n",
       "v   0  \n",
       "w   0  \n",
       "y   0  \n",
       "z  62  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "conf_mat = pandas.DataFrame(confusion_matrix(ytrain[train_index], y_train_pred))\n",
    "conf_mat.columns = key\n",
    "conf_mat.index = key\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Observations of validations confusion matrix\n",
    "* h gets mixed up with n, w\n",
    "* m gets mixed up with m, u, w\n",
    "* n gets mixed up with m, w\n",
    "* u gets mixed up with w\n",
    "\n",
    "### Observations of training confusion matrix\n",
    "* h gets mixed up with n, w\n",
    "* m gets mixed up with u, w\n",
    "* n gets mixed up with n, w, m\n",
    "* p gets mixed up with n\n",
    "* u gets mixed up with a, w\n",
    "* w gets mixed up with a, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate a random sample from the gmm for a certain class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc2de99ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myClass = [20]\n",
    "x = hmm_classifier.generateSample(label_enc.transform(myClass)[0], 200)\n",
    "x = x * (train_max - train_min) + train_min\n",
    "plot_char(x.T, myClass[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Basic Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy', 5, 0.7998600419874038)\n",
      "('Accuracy', 6, 0.8621413575927221)\n",
      "('Accuracy', 7, 0.8530440867739678)\n",
      "('Accuracy', 8, 0.8761371588523443)\n",
      "('Accuracy', 9, 0.8803358992302309)\n",
      "('Accuracy', 10, 0.8999300209937019)\n",
      "('Accuracy', 11, 0.900629811056683)\n",
      "('Accuracy', 12, 0.9062281315605318)\n",
      "('Accuracy', 13, 0.9097270818754374)\n",
      "('Accuracy', 14, 0.9111266620013996)\n",
      "('Accuracy', 15, 0.9188243526941917)\n",
      "('Accuracy', 16, 0.9118264520643807)\n",
      "('Accuracy', 17, 0.9132260321903429)\n",
      "('Accuracy', 18, 0.9111266620013996)\n",
      "('Accuracy', 19, 0.9104268719384184)\n",
      "('Accuracy', 20, 0.9146256123163051)\n",
      "('Accuracy', 21, 0.9125262421273618)\n",
      "('Accuracy', 22, 0.9181245626312107)\n",
      "('Accuracy', 23, 0.9230230930720784)\n",
      "('Accuracy', 24, 0.9258222533240028)\n",
      "('Accuracy', 25, 0.9258222533240028)\n",
      "('Accuracy', 26, 0.9153254023792862)\n",
      "('Accuracy', 27, 0.9230230930720784)\n",
      "('Accuracy', 28, 0.9286214135759272)\n",
      "('Accuracy', 29, 0.9209237228831351)\n",
      "('Accuracy', 30, 0.9258222533240028)\n"
     ]
    }
   ],
   "source": [
    "cv1_results = {}\n",
    "for k in range(5, 31):\n",
    "    pi0 = np.eye(1, k)[0]\n",
    "    trans0 = np.diag(np.ones(k)) + np.diag(np.ones(k-1), 1)\n",
    "    trans0 /= trans0.sum(axis=1).reshape(-1, 1)\n",
    "    hmm = GaussianHMM(n_components=k, \n",
    "#                       init_params='mc',\n",
    "                      n_iter=10,\n",
    "                      random_state=seed)\n",
    "    correct = 0.0\n",
    "    for train_index, test_index in kf.split(xtrain, ytrain):\n",
    "#         hmm.startprob_ = pi0\n",
    "#         hmm.transmat_  = trans0\n",
    "        hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "        hmm_classifier.fit(xtrain[train_index], label_enc.transform(ytrain[train_index]), np.tile(k, 20))\n",
    "        y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain[test_index]))\n",
    "        correct += np.sum(y_val_pred == ytrain[test_index])\n",
    "    accuracy = correct/xtrain.shape[0]\n",
    "    cv1_results[k] = accuracy\n",
    "    print('Accuracy', k, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv2_results = {}\n",
    "for k in range(5, 31):\n",
    "    pi0 = np.eye(1, k)[0]\n",
    "    trans0 = np.diag(np.ones(k)) + np.diag(np.ones(k-1), 1)\n",
    "    trans0 /= trans0.sum(axis=1).reshape(-1, 1)\n",
    "    hmm = GaussianHMM(n_components=k,\n",
    "                      n_iter=10,\n",
    "                      random_state=seed)\n",
    "    class_cond_accuracies = {}\n",
    "    for train_index, test_index in kf.split(xtrain, ytrain):\n",
    "        hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "        hmm_classifier.fit(xtrain[train_index], label_enc.transform(ytrain[train_index]), np.tile(k, 20))\n",
    "        for label in label_enc.classes_:\n",
    "            class_cond_xtest = xtrain[test_index][ytrain[test_index] == label]\n",
    "            y_class_cond_pred = label_enc.inverse_transform(hmm_classifier.predict(class_cond_xtest))\n",
    "            class_cond_accuracy = (y_class_cond_pred == label).mean()\n",
    "            if (not class_cond_accuracies.has_key(label)):\n",
    "                class_cond_accuracies[label] = []\n",
    "            class_cond_accuracies[label] = class_cond_accuracies[label] + [class_cond_accuracy]\n",
    "\n",
    "    k_states_results = {}\n",
    "    for label in label_enc.classes_:\n",
    "        k_states_results[label] = np.mean(class_cond_accuracies[label])\n",
    "    cv2_results[k] = k_states_results\n",
    "    print('Average for k = ', k, np.mean(cv2_results[k].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9519</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.9733</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.7977</td>\n",
       "      <td>0.5507</td>\n",
       "      <td>0.4349</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.8720</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>0.7044</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4848</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9638</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4386</td>\n",
       "      <td>0.8234</td>\n",
       "      <td>0.7161</td>\n",
       "      <td>0.4849</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.7404</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.6719</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9048</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.7972</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.7746</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.2973</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.8614</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.8613</td>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.8096</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5635</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8789</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2456</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>0.5016</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.8263</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.5332</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8956</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>0.9103</td>\n",
       "      <td>0.6403</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.8605</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6263</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8439</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4737</td>\n",
       "      <td>0.8599</td>\n",
       "      <td>0.6416</td>\n",
       "      <td>0.5635</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.6732</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8623</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4912</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>0.6713</td>\n",
       "      <td>0.5976</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.6861</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8789</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.8865</td>\n",
       "      <td>0.6713</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8614</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4386</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>0.6858</td>\n",
       "      <td>0.6452</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.7965</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5263</td>\n",
       "      <td>0.8984</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.6286</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8124</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.8798</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5088</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>0.6421</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7489</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.8456</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5088</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7338</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.8439</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3684</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.6444</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7799</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3684</td>\n",
       "      <td>0.8723</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4035</td>\n",
       "      <td>0.8732</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.6921</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8139</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.8465</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7626</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8623</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4386</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>0.6397</td>\n",
       "      <td>0.5794</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.8640</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>0.6778</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>0.6568</td>\n",
       "      <td>0.7270</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>0.7103</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4386</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.6302</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8117</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4386</td>\n",
       "      <td>0.9112</td>\n",
       "      <td>0.6864</td>\n",
       "      <td>0.5976</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8593</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9149</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5439</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>0.6258</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9149</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>0.6468</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8593</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4737</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>0.7161</td>\n",
       "      <td>0.6294</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9149</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         a       b       c       d       e       g       h       l       m  \\\n",
       "5   0.9519  0.9524  0.9091  0.2240  0.9896  0.9733  0.1930  0.7977  0.5507   \n",
       "6   0.9638  0.9167  0.9545  1.0000  0.9896  1.0000  0.4386  0.8234  0.7161   \n",
       "7   0.9757  0.9048  0.9848  0.9855  0.9896  1.0000  0.3158  0.7972  0.5527   \n",
       "8   0.9877  0.9524  0.9848  1.0000  0.9896  1.0000  0.2281  0.8613  0.5672   \n",
       "9   0.9757  0.9643  1.0000  1.0000  0.9896  1.0000  0.2456  0.8979  0.6555   \n",
       "10  0.9877  0.9643  1.0000  1.0000  0.9896  1.0000  0.4561  0.9103  0.6403   \n",
       "11  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.4737  0.8599  0.6416   \n",
       "12  1.0000  0.9405  1.0000  1.0000  0.9896  1.0000  0.4912  0.8727  0.6713   \n",
       "13  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.3860  0.8865  0.6713   \n",
       "14  0.9877  0.9405  1.0000  1.0000  0.9896  1.0000  0.4386  0.8481  0.6858   \n",
       "15  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.5263  0.8984  0.6548   \n",
       "16  1.0000  0.9405  1.0000  1.0000  0.9896  1.0000  0.5088  0.8471  0.6555   \n",
       "17  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.5088  0.8727  0.6410   \n",
       "18  0.9877  0.9643  1.0000  1.0000  0.9896  1.0000  0.3684  0.8471  0.6561   \n",
       "19  0.9877  0.9643  0.9848  1.0000  0.9896  1.0000  0.3684  0.8723  0.6700   \n",
       "20  0.9877  0.9643  1.0000  1.0000  0.9896  1.0000  0.4035  0.8732  0.6133   \n",
       "21  0.9877  0.9643  1.0000  1.0000  0.9896  1.0000  0.3860  0.9107  0.6561   \n",
       "22  0.9877  0.9762  1.0000  1.0000  0.9896  1.0000  0.4386  0.8979  0.6397   \n",
       "23  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.4561  0.9107  0.6555   \n",
       "24  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.4561  0.8979  0.6568   \n",
       "25  0.9877  0.9405  1.0000  1.0000  0.9896  1.0000  0.4561  0.8856  0.7003   \n",
       "26  0.9877  0.9286  1.0000  1.0000  0.9896  1.0000  0.4386  0.8727  0.6423   \n",
       "27  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.4386  0.9112  0.6864   \n",
       "28  0.9877  0.9643  1.0000  1.0000  0.9896  1.0000  0.5439  0.9236  0.6258   \n",
       "29  0.9877  0.9524  1.0000  1.0000  0.9896  1.0000  0.3860  0.8979  0.6555   \n",
       "30  0.9877  0.9286  1.0000  1.0000  0.9896  1.0000  0.4737  0.9107  0.7161   \n",
       "\n",
       "         n       o       p       q       r       s       u       v       w  \\\n",
       "5   0.4349  0.9848  0.8720  0.8246  0.7044  1.0000  0.4848  0.9889  0.7254   \n",
       "6   0.4849  0.9545  0.9281  0.8947  0.7404  1.0000  0.4394  0.9889  0.6719   \n",
       "7   0.4683  0.9697  0.9281  0.9474  0.7746  0.9848  0.2973  0.9889  0.8614   \n",
       "8   0.5167  0.9697  0.9275  0.9649  0.8096  1.0000  0.5635  1.0000  0.8789   \n",
       "9   0.5016  0.9848  0.9143  0.9298  0.8263  0.9848  0.5332  1.0000  0.8956   \n",
       "10  0.5643  0.9848  0.9710  0.9474  0.8605  1.0000  0.6263  1.0000  0.8439   \n",
       "11  0.5635  0.9848  0.9432  0.9649  0.9307  0.9690  0.6732  1.0000  0.8623   \n",
       "12  0.5976  0.9848  0.9432  0.9474  0.8965  0.9848  0.6861  1.0000  0.8789   \n",
       "13  0.6786  0.9848  0.9432  0.9474  0.9307  1.0000  0.7338  1.0000  0.8614   \n",
       "14  0.6452  1.0000  0.9287  0.9474  0.9298  0.9683  0.7965  1.0000  0.8965   \n",
       "15  0.6286  1.0000  0.9565  0.9474  0.9649  0.9841  0.8124  0.9889  0.8798   \n",
       "16  0.6421  0.9848  0.9710  0.9474  0.9649  0.9841  0.7489  0.9889  0.8456   \n",
       "17  0.6929  0.9848  0.9710  0.9474  0.9474  0.9841  0.7338  0.9889  0.8439   \n",
       "18  0.6444  0.9848  0.9855  0.9474  0.9482  0.9841  0.7799  1.0000  0.8974   \n",
       "19  0.6278  1.0000  1.0000  0.9649  0.9474  0.9841  0.7345  0.9889  0.8807   \n",
       "20  0.6921  1.0000  0.9855  0.9649  0.9649  0.9841  0.8139  0.9889  0.8465   \n",
       "21  0.5952  1.0000  0.9710  0.9474  0.9649  1.0000  0.7626  1.0000  0.8623   \n",
       "22  0.5794  1.0000  1.0000  0.9649  0.9649  0.9841  0.8579  0.9889  0.8640   \n",
       "23  0.6778  1.0000  1.0000  0.9649  0.9474  0.9841  0.8276  1.0000  0.8982   \n",
       "24  0.7270  1.0000  0.9855  0.9825  0.9649  0.9841  0.8427  1.0000  0.8982   \n",
       "25  0.7103  1.0000  1.0000  0.9825  0.9649  0.9841  0.8283  1.0000  0.8982   \n",
       "26  0.6302  1.0000  0.9710  0.9825  0.9307  0.9841  0.8117  1.0000  0.9316   \n",
       "27  0.5976  1.0000  1.0000  0.9825  0.9482  0.9841  0.8593  1.0000  0.9149   \n",
       "28  0.6944  1.0000  0.9855  1.0000  0.9825  0.9841  0.7980  1.0000  0.9149   \n",
       "29  0.6468  1.0000  1.0000  0.9649  0.9482  0.9841  0.8593  1.0000  0.9316   \n",
       "30  0.6294  1.0000  1.0000  0.9825  0.9825  0.9841  0.8283  1.0000  0.9149   \n",
       "\n",
       "         y    z  \n",
       "5   1.0000  1.0  \n",
       "6   0.9855  1.0  \n",
       "7   0.9855  1.0  \n",
       "8   0.9855  1.0  \n",
       "9   0.9710  1.0  \n",
       "10  0.9710  1.0  \n",
       "11  0.9710  1.0  \n",
       "12  1.0000  1.0  \n",
       "13  1.0000  1.0  \n",
       "14  1.0000  1.0  \n",
       "15  1.0000  1.0  \n",
       "16  1.0000  1.0  \n",
       "17  1.0000  1.0  \n",
       "18  1.0000  1.0  \n",
       "19  1.0000  1.0  \n",
       "20  1.0000  1.0  \n",
       "21  1.0000  1.0  \n",
       "22  1.0000  1.0  \n",
       "23  1.0000  1.0  \n",
       "24  1.0000  1.0  \n",
       "25  1.0000  1.0  \n",
       "26  1.0000  1.0  \n",
       "27  1.0000  1.0  \n",
       "28  1.0000  1.0  \n",
       "29  1.0000  1.0  \n",
       "30  1.0000  1.0  "
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pandas.DataFrame(cv2_results).T\n",
    "result.columns = key\n",
    "result.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 22,  9,  6,  5,  6, 28, 28,  6, 24, 14, 19, 28, 28,  5, 27,  8,\n",
       "       26,  5,  5])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Highest n_states for each character\n",
    "np.asarray(range(5, 31))[np.argmax(np.asarray(result), axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tested states\n",
    "# [28, 28, 28, 28, 28, 28, 28, 28, 30, 20, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28] 0.939xx\n",
    "# [28, 28, 28, 28, 28, 28, 28, 28, 25, 20, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28] 0.93983402489626555\n",
    "# [28, 28, 28, 28, 28, 28, 28, 28, 25, 25, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28] 0.94190871369294604\n",
    "# [28, 28, 28, 28, 28, 28, 28, 28, 25, 25, 28, 28, 28, 28, 28, 15, 28, 26, 28, 28] 0.91701244813278004\n",
    "# [28, 28, 28, 28, 28, 28, 28, 10, 25, 25, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28] 0.91xx\n",
    "# [28, 28, 28, 28, 28, 28, 28, 21, 25, 25, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28] 0.94190871369294604\n",
    "# [28, 28, 28, 28, 28, 28, 15, 28, 25, 25, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28] 0.92323651452282163\n",
    "\n",
    "# Winner\n",
    "# [28, 28, 28, 28, 28, 28, 28, 28, 25, 25, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Retraining the classifier using the \"Optimized\" number of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerativeClassifierHMM(hmm=GaussianHMM(algorithm='viterbi', covariance_type='diag', covars_prior=0.01,\n",
       "      covars_weight=1, init_params='stmc', means_prior=0, means_weight=0,\n",
       "      min_covar=0.001, n_components=25, n_iter=10, params='stmc',\n",
       "      random_state=1234, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
       "      verbose=False))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "train_index, test_index = list(kf.split(xtrain, ytrain))[0]\n",
    "\n",
    "hmm_classifier.fit(xtrain[train_index], \n",
    "                   label_enc.transform(ytrain[train_index]),\n",
    "                   np.array([28, 28, 28, 28, 28, 28, 28, 28, 25, 25, 28, 28, 28, 28, 28, 27, 28, 26, 28, 28])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Validation Accuracy', 0.94190871369294604)\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy', (y_val_pred == ytrain[test_index]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain[train_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Accuracy', 0.96620908130939809)\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy', (y_train_pred == ytrain[train_index]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Gaussian Mixture HMM (meh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter, freqz\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    filtered = np.empty_like(data)\n",
    "    \n",
    "    for i in range(filtered.shape[0]):\n",
    "        ll = np.zeros(data[i].shape)\n",
    "        \n",
    "        for j in range(3):\n",
    "            ll[:,j] = lfilter(b, a, data[i][:,j])\n",
    "        \n",
    "        filtered[i] = ll\n",
    "    return filtered\n",
    "\n",
    "# Filter requirements.\n",
    "order = 4\n",
    "fs = 200.0       # sample rate, Hz\n",
    "cutoff = 2  # desired cutoff frequency of the filter, Hz\n",
    "\n",
    "xtrain_filtered = butter_lowpass_filter(xtrain, cutoff, fs, order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_states = 10\n",
    "\n",
    "# initial guess for EM\n",
    "# pi0 = np.eye(1, n_states)[0] # start probability\n",
    "# pi0\n",
    "\n",
    "# initial guess for EM\n",
    "# transition matrix\n",
    "# trans0 = np.diag(np.ones(n_states)) + np.diag(np.ones(n_states-1), 1)\n",
    "# trans0 /= trans0.sum(axis=1).reshape(-1, 1)\n",
    "# trans0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hmm = GaussianHMM(n_components=n_states, \n",
    "#                   init_params='mc',\n",
    "                  n_iter=10,\n",
    "                  random_state=seed)\n",
    "# hmm.startprob_ = pi0\n",
    "# hmm.transmat_  = trans0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# due to https://github.com/hmmlearn/hmmlearn/issues/158 and \n",
    "# https://github.com/hmmlearn/hmmlearn/issues/175\n",
    "# the fitting process is going to give a LOT of warnings\n",
    "# so we hide them in this notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerativeClassifierHMM(hmm=GaussianHMM(algorithm='viterbi', covariance_type='diag', covars_prior=0.01,\n",
       "      covars_weight=1, init_params='stmc', means_prior=0, means_weight=0,\n",
       "      min_covar=0.001, n_components=10, n_iter=10, params='stmc',\n",
       "      random_state=1234, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
       "      verbose=False))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "train_index, test_index = list(kf.split(xtrain, ytrain))[0]\n",
    "\n",
    "hmm_classifier.fit(xtrain_filtered[train_index], \n",
    "                   label_enc.transform(ytrain[train_index]),\n",
    "                   np.tile(15, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Validation Accuracy', 0.96887966804979253)\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy', (y_val_pred == ytrain[test_index]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[train_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Accuracy', 0.96409714889123543)\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy', (y_train_pred == ytrain[train_index]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation on number of states, cutoff frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "order = 4\n",
    "fs = 200.0  \n",
    "\n",
    "hmm = GaussianHMM(n_components=n_states,\n",
    "                  n_iter=10,\n",
    "                  random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: k = 10; cut_freq = 2 ---> 0.941175230565\n",
      "Validation Accuracy: k = 10; cut_freq = 3 ---> 0.974032995778\n",
      "Validation Accuracy: k = 10; cut_freq = 4 ---> 0.951644298248\n",
      "Validation Accuracy: k = 10; cut_freq = 5 ---> 0.951651732219\n",
      "Validation Accuracy: k = 10; cut_freq = 6 ---> 0.943244786528\n",
      "Validation Accuracy: k = 10; cut_freq = 7 ---> 0.929225864831\n",
      "Validation Accuracy: k = 10; cut_freq = 8 ---> 0.915280264919\n",
      "Validation Accuracy: k = 10; cut_freq = 9 ---> 0.906148170785\n",
      "Validation Accuracy: k = 10; cut_freq = 10 ---> 0.906148170785\n",
      "Validation Accuracy: k = 11; cut_freq = 2 ---> 0.947467751185\n",
      "Validation Accuracy: k = 11; cut_freq = 3 ---> 0.965722956995\n",
      "Validation Accuracy: k = 11; cut_freq = 4 ---> 0.956534975435\n",
      "Validation Accuracy: k = 11; cut_freq = 5 ---> 0.954533608424\n",
      "Validation Accuracy: k = 11; cut_freq = 6 ---> 0.947453901172\n",
      "Validation Accuracy: k = 11; cut_freq = 7 ---> 0.934897577887\n",
      "Validation Accuracy: k = 11; cut_freq = 8 ---> 0.925800352473\n",
      "Validation Accuracy: k = 11; cut_freq = 9 ---> 0.908948008025\n",
      "Validation Accuracy: k = 11; cut_freq = 10 ---> 0.914540248533\n",
      "Validation Accuracy: k = 12; cut_freq = 2 ---> 0.950240153677\n",
      "Validation Accuracy: k = 12; cut_freq = 3 ---> 0.965691937901\n",
      "Validation Accuracy: k = 12; cut_freq = 4 ---> 0.951664299024\n",
      "Validation Accuracy: k = 12; cut_freq = 5 ---> 0.959375832158\n",
      "Validation Accuracy: k = 12; cut_freq = 6 ---> 0.95101733992\n",
      "Validation Accuracy: k = 12; cut_freq = 7 ---> 0.93974108483\n",
      "Validation Accuracy: k = 12; cut_freq = 8 ---> 0.916679541935\n",
      "Validation Accuracy: k = 12; cut_freq = 9 ---> 0.908313615727\n",
      "Validation Accuracy: k = 12; cut_freq = 10 ---> 0.900598232968\n",
      "Validation Accuracy: k = 13; cut_freq = 2 ---> 0.950207851374\n",
      "Validation Accuracy: k = 13; cut_freq = 3 ---> 0.964266509345\n",
      "Validation Accuracy: k = 13; cut_freq = 4 ---> 0.953093311925\n",
      "Validation Accuracy: k = 13; cut_freq = 5 ---> 0.958015008207\n",
      "Validation Accuracy: k = 13; cut_freq = 6 ---> 0.946819508873\n",
      "Validation Accuracy: k = 13; cut_freq = 7 ---> 0.946053340948\n",
      "Validation Accuracy: k = 13; cut_freq = 8 ---> 0.925056486462\n",
      "Validation Accuracy: k = 13; cut_freq = 9 ---> 0.913143272656\n",
      "Validation Accuracy: k = 13; cut_freq = 10 ---> 0.909690590828\n",
      "Validation Accuracy: k = 14; cut_freq = 2 ---> 0.952339710805\n",
      "Validation Accuracy: k = 14; cut_freq = 3 ---> 0.963542644111\n",
      "Validation Accuracy: k = 14; cut_freq = 4 ---> 0.96007611227\n",
      "Validation Accuracy: k = 14; cut_freq = 5 ---> 0.95376615729\n",
      "Validation Accuracy: k = 14; cut_freq = 6 ---> 0.934873992765\n",
      "Validation Accuracy: k = 14; cut_freq = 7 ---> 0.936988417835\n",
      "Validation Accuracy: k = 14; cut_freq = 8 ---> 0.929233298803\n",
      "Validation Accuracy: k = 14; cut_freq = 9 ---> 0.914592286334\n",
      "Validation Accuracy: k = 14; cut_freq = 10 ---> 0.906136887189\n",
      "Validation Accuracy: k = 15; cut_freq = 2 ---> 0.958613513857\n",
      "Validation Accuracy: k = 15; cut_freq = 3 ---> 0.962877232718\n",
      "Validation Accuracy: k = 15; cut_freq = 4 ---> 0.961498974408\n",
      "Validation Accuracy: k = 15; cut_freq = 5 ---> 0.958665551657\n",
      "Validation Accuracy: k = 15; cut_freq = 6 ---> 0.948134180507\n",
      "Validation Accuracy: k = 15; cut_freq = 7 ---> 0.939724933678\n",
      "Validation Accuracy: k = 15; cut_freq = 8 ---> 0.923649775474\n",
      "Validation Accuracy: k = 15; cut_freq = 9 ---> 0.915982846169\n",
      "Validation Accuracy: k = 15; cut_freq = 10 ---> 0.908246444705\n",
      "Validation Accuracy: k = 16; cut_freq = 2 ---> 0.952340994013\n",
      "Validation Accuracy: k = 16; cut_freq = 3 ---> 0.96779021182\n",
      "Validation Accuracy: k = 16; cut_freq = 4 ---> 0.959440436763\n",
      "Validation Accuracy: k = 16; cut_freq = 5 ---> 0.959365831769\n",
      "Validation Accuracy: k = 16; cut_freq = 6 ---> 0.950268606355\n",
      "Validation Accuracy: k = 16; cut_freq = 7 ---> 0.934887577499\n",
      "Validation Accuracy: k = 16; cut_freq = 8 ---> 0.924373640709\n",
      "Validation Accuracy: k = 16; cut_freq = 9 ---> 0.911061149887\n",
      "Validation Accuracy: k = 16; cut_freq = 10 ---> 0.9061704727\n",
      "Validation Accuracy: k = 17; cut_freq = 2 ---> 0.957263973504\n",
      "Validation Accuracy: k = 17; cut_freq = 3 ---> 0.967066346585\n",
      "Validation Accuracy: k = 17; cut_freq = 4 ---> 0.962868515538\n",
      "Validation Accuracy: k = 17; cut_freq = 5 ---> 0.963576229621\n",
      "Validation Accuracy: k = 17; cut_freq = 6 ---> 0.950961452495\n",
      "Validation Accuracy: k = 17; cut_freq = 7 ---> 0.938438714722\n",
      "Validation Accuracy: k = 17; cut_freq = 8 ---> 0.92018682798\n",
      "Validation Accuracy: k = 17; cut_freq = 9 ---> 0.917375972422\n",
      "Validation Accuracy: k = 17; cut_freq = 10 ---> 0.918098554449\n",
      "Validation Accuracy: k = 18; cut_freq = 2 ---> 0.962813911321\n",
      "Validation Accuracy: k = 18; cut_freq = 3 ---> 0.966383500833\n",
      "Validation Accuracy: k = 18; cut_freq = 4 ---> 0.962859798358\n",
      "Validation Accuracy: k = 18; cut_freq = 5 ---> 0.961472822869\n",
      "Validation Accuracy: k = 18; cut_freq = 6 ---> 0.951650449011\n",
      "Validation Accuracy: k = 18; cut_freq = 7 ---> 0.934917313385\n",
      "Validation Accuracy: k = 18; cut_freq = 8 ---> 0.913864571474\n",
      "Validation Accuracy: k = 18; cut_freq = 9 ---> 0.916732862944\n",
      "Validation Accuracy: k = 18; cut_freq = 10 ---> 0.90629096473\n",
      "Validation Accuracy: k = 19; cut_freq = 2 ---> 0.957915800162\n",
      "Validation Accuracy: k = 19; cut_freq = 3 ---> 0.96713095119\n",
      "Validation Accuracy: k = 19; cut_freq = 4 ---> 0.969141035382\n",
      "Validation Accuracy: k = 19; cut_freq = 5 ---> 0.962865949121\n",
      "Validation Accuracy: k = 19; cut_freq = 6 ---> 0.949641648027\n",
      "Validation Accuracy: k = 19; cut_freq = 7 ---> 0.942009587442\n",
      "Validation Accuracy: k = 19; cut_freq = 8 ---> 0.927185779475\n",
      "Validation Accuracy: k = 19; cut_freq = 9 ---> 0.923761550325\n",
      "Validation Accuracy: k = 19; cut_freq = 10 ---> 0.914577418391\n",
      "Validation Accuracy: k = 20; cut_freq = 2 ---> 0.96213849954\n",
      "Validation Accuracy: k = 20; cut_freq = 3 ---> 0.968483057961\n",
      "Validation Accuracy: k = 20; cut_freq = 4 ---> 0.958675552045\n",
      "Validation Accuracy: k = 20; cut_freq = 5 ---> 0.964226773071\n",
      "Validation Accuracy: k = 20; cut_freq = 6 ---> 0.943334259464\n",
      "Validation Accuracy: k = 20; cut_freq = 7 ---> 0.935681180173\n",
      "Validation Accuracy: k = 20; cut_freq = 8 ---> 0.927872474853\n",
      "Validation Accuracy: k = 20; cut_freq = 9 ---> 0.922235630516\n",
      "Validation Accuracy: k = 20; cut_freq = 10 ---> 0.920955562322\n",
      "Validation Accuracy: k = 21; cut_freq = 2 ---> 0.959332511537\n",
      "Validation Accuracy: k = 21; cut_freq = 3 ---> 0.969215640375\n",
      "Validation Accuracy: k = 21; cut_freq = 4 ---> 0.953752572555\n",
      "Validation Accuracy: k = 21; cut_freq = 5 ---> 0.96219925452\n",
      "Validation Accuracy: k = 21; cut_freq = 6 ---> 0.946140247467\n",
      "Validation Accuracy: k = 21; cut_freq = 7 ---> 0.948900348433\n",
      "Validation Accuracy: k = 21; cut_freq = 8 ---> 0.92569087876\n",
      "Validation Accuracy: k = 21; cut_freq = 9 ---> 0.932050570402\n",
      "Validation Accuracy: k = 21; cut_freq = 10 ---> 0.925059052878\n",
      "Validation Accuracy: k = 22; cut_freq = 2 ---> 0.95935609666\n",
      "Validation Accuracy: k = 22; cut_freq = 3 ---> 0.966389651596\n",
      "Validation Accuracy: k = 22; cut_freq = 4 ---> 0.956552409795\n",
      "Validation Accuracy: k = 22; cut_freq = 5 ---> 0.96635119853\n",
      "Validation Accuracy: k = 22; cut_freq = 6 ---> 0.951653015427\n",
      "Validation Accuracy: k = 22; cut_freq = 7 ---> 0.941146512609\n",
      "Validation Accuracy: k = 22; cut_freq = 8 ---> 0.929210996888\n",
      "Validation Accuracy: k = 22; cut_freq = 9 ---> 0.930658727358\n",
      "Validation Accuracy: k = 22; cut_freq = 10 ---> 0.939046955481\n",
      "Validation Accuracy: k = 23; cut_freq = 2 ---> 0.958648382576\n",
      "Validation Accuracy: k = 23; cut_freq = 3 ---> 0.968506643083\n",
      "Validation Accuracy: k = 23; cut_freq = 4 ---> 0.963558795262\n",
      "Validation Accuracy: k = 23; cut_freq = 5 ---> 0.963608531924\n",
      "Validation Accuracy: k = 23; cut_freq = 6 ---> 0.957260123878\n",
      "Validation Accuracy: k = 23; cut_freq = 7 ---> 0.946749771435\n",
      "Validation Accuracy: k = 23; cut_freq = 8 ---> 0.932065438345\n",
      "Validation Accuracy: k = 23; cut_freq = 9 ---> 0.933520602786\n",
      "Validation Accuracy: k = 23; cut_freq = 10 ---> 0.939131560862\n",
      "Validation Accuracy: k = 24; cut_freq = 2 ---> 0.960056376772\n",
      "Validation Accuracy: k = 24; cut_freq = 3 ---> 0.970556463549\n",
      "Validation Accuracy: k = 24; cut_freq = 4 ---> 0.965650918418\n",
      "Validation Accuracy: k = 24; cut_freq = 5 ---> 0.966398368776\n",
      "Validation Accuracy: k = 24; cut_freq = 6 ---> 0.953751289347\n",
      "Validation Accuracy: k = 24; cut_freq = 7 ---> 0.943978652151\n",
      "Validation Accuracy: k = 24; cut_freq = 8 ---> 0.936264552601\n",
      "Validation Accuracy: k = 24; cut_freq = 9 ---> 0.92081891914\n",
      "Validation Accuracy: k = 24; cut_freq = 10 ---> 0.939096692143\n",
      "Validation Accuracy: k = 25; cut_freq = 2 ---> 0.962847496832\n",
      "Validation Accuracy: k = 25; cut_freq = 3 ---> 0.967089931708\n",
      "Validation Accuracy: k = 25; cut_freq = 4 ---> 0.962175669397\n",
      "Validation Accuracy: k = 25; cut_freq = 5 ---> 0.963598531536\n",
      "Validation Accuracy: k = 25; cut_freq = 6 ---> 0.950977603646\n",
      "Validation Accuracy: k = 25; cut_freq = 7 ---> 0.939176164691\n"
     ]
    }
   ],
   "source": [
    "cv3_results = {}\n",
    "for k in range(10, 31):\n",
    "    for cutoff in [2, 3, 4, 5, 6, 7, 8, 9, 10]:  # desired cutoff frequency of the filter, Hz\n",
    "        xtrain_filtered = butter_lowpass_filter(xtrain, cutoff, fs, order)\n",
    "        val_acc = 0.0\n",
    "        tra_acc = 0.0\n",
    "        for train_index, test_index in kf.split(xtrain, ytrain):\n",
    "            hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "            hmm_classifier.fit(xtrain_filtered[train_index], \n",
    "                       label_enc.transform(ytrain[train_index]),\n",
    "                       np.tile(k, 20))\n",
    "\n",
    "            y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[test_index]))\n",
    "            val_acc += (y_val_pred == ytrain[test_index]).mean()\n",
    "            \n",
    "            y_train_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[train_index]))\n",
    "            \n",
    "            tra_acc += (y_train_pred == ytrain[train_index]).mean()\n",
    "            \n",
    "        val_acc /= kf.get_n_splits()\n",
    "        print('Validation Accuracy: k = ' + str(k) + '; cut_freq = ' + str(cutoff) + ' ---> ' + str(val_acc))\n",
    "        \n",
    "        cv3_results[(k, cutoff)] = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Gaussian Mixture HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note**:\n",
    "I was getting some results (70%) with these parameters when I was on version 0.2.0 but after updating to the latest\n",
    "version (0.2.1), GMMHMM hasn't been great and it's taking too long to run. There are some open issues on their Github which seem to suggest GMMHMM is a bit buggy atm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0. ],\n",
       "       [ 0. ,  0.5,  0.5],\n",
       "       [ 0. ,  0. ,  1. ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "n_states = 3\n",
    "n_mix = 10\n",
    "# initial guess for EM\n",
    "pi0 = np.eye(1, n_states)[0] # start probability\n",
    "pi0\n",
    "\n",
    "# initial guess for EM\n",
    "# transition matrix\n",
    "trans0 = np.diag(np.ones(n_states)) + np.diag(np.ones(n_states-1), 1)\n",
    "trans0 /= trans0.sum(axis=1).reshape(-1, 1)\n",
    "trans0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gmmhmm = GMMHMM(n_components=n_states, \n",
    "                n_mix=n_mix,\n",
    "                covariance_type='diag',\n",
    "                init_params='mc',\n",
    "                n_iter=3,\n",
    "                random_state=rng)\n",
    "gmmhmm.startprob_ = pi0\n",
    "gmmhmm.transmat_  = trans0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerativeClassifierHMM(hmm=GMMHMM(algorithm='viterbi', covariance_type='diag', covars_prior=None,\n",
       "    covars_weight=None, init_params='mc', means_prior=0.0,\n",
       "    means_weight=0.0, min_covar=0.001, n_components=3, n_iter=3, n_mix=10,\n",
       "    params='stmcw',\n",
       "    random_state=<mtrand.RandomState object at 0x000000000A48A3A8>,\n",
       "    startprob_prior=1.0, tol=0.01, transmat_prior=1.0, verbose=False,\n",
       "    weights_prior=1.0))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_classifier = GenerativeClassifierHMM(gmmhmm)\n",
    "hmm_classifier.fit(xtrain, label_enc.transform(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20L, 429L)\n",
      "(20L,)\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy', 0.38694638694638694)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy', (y_val_pred == yval).mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Basic Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Guassian Mixture Classifier (Zero Padded Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Padding sequances with zero so they all have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pad_seq(seq):\n",
    "    seq_len = seq.shape[0]\n",
    "    x = np.pad(seq[:, 0], (0, max_length - seq_len%max_length), 'constant')\n",
    "    y = np.pad(seq[:, 1], (0, max_length - seq_len%max_length), 'constant')\n",
    "    z = np.pad(seq[:, 2], (0, max_length - seq_len%max_length), 'constant')\n",
    "    return np.stack((x, y, z)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xtrain_zp = np.asarray([pad_seq(seq) for seq in xtrain])\n",
    "xtest_zp = np.asarray([pad_seq(seq) for seq in xtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1429, 250, 3)\n",
      "(1429, 250, 3)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_zp.shape)\n",
    "print(xtest_zp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureClassifier():\n",
    "    '''\n",
    "    GaussianMixtureClassifier is classifier where a seperate GMM is trained on\n",
    "    different classes and it has a similar API to ski-learn classifiers\n",
    "    Parameters\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        K = int(np.max(y)+1)\n",
    "        self.models = []\n",
    "        for k in range(K):\n",
    "            model = GaussianMixture(**self.kwargs)\n",
    "            model.fit(X[y==k])\n",
    "            self.models.append(model)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        loglikelihood = [ model.score_samples(X) for model in self.models ]\n",
    "        return np.vstack(loglikelihood).argmax(axis=0)\n",
    "    \n",
    "    def score(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        loglikelihood = [ model.score_samples(X) for model in self.models ]\n",
    "        return np.vstack(loglikelihood).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reshaping the data into a two dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xtrain_zp = xtrain_zp.reshape(xtrain_zp.shape[0], xtrain_zp.shape[1]* xtrain_zp.shape[2])\n",
    "ytrain_zp = ytrain - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1429, 750)\n",
      "set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_zp.shape)\n",
    "print(set(ytrain_zp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Diagonal Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.96680497925311204)\n",
      "('Accuracy: ', 0.97268907563025209)\n",
      "('Accuracy: ', 0.97027600849256901)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(xtrain, ytrain):\n",
    "    gmm_classifier = GaussianMixtureClassifier(n_components=10, covariance_type='diag', verbose=False, max_iter=1000)\n",
    "\n",
    "    gmm_classifier.fit(xtrain_zp[train_index], ytrain[train_index]-1)\n",
    "    accuracy = (gmm_classifier.predict(xtrain_zp[test_index]) == (ytrain[test_index] - 1)).mean()\n",
    "    print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Full Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 1.0)\n",
      "('Accuracy: ', 0.99159663865546221)\n",
      "('Accuracy: ', 0.99150743099787686)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(xtrain, ytrain):\n",
    "    gmm_classifier = GaussianMixtureClassifier(n_components=10, covariance_type='full', verbose=False, max_iter=1000)\n",
    "    gmm_classifier.fit(xtrain_zp[train_index], ytrain[train_index]-1)\n",
    "    accuracy = (gmm_classifier.predict(xtrain_zp[test_index]) == (ytrain[test_index] - 1)).mean()\n",
    "    print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Low-pass Filtering of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter, freqz\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    filtered = np.empty_like(data)\n",
    "    \n",
    "    for i in range(filtered.shape[0]):\n",
    "        ll = np.zeros(data[i].shape)\n",
    "        \n",
    "        for j in range(3):\n",
    "            ll[:,j] = lfilter(b, a, data[i][:,j])\n",
    "        \n",
    "        filtered[i] = ll\n",
    "    return filtered\n",
    "\n",
    "# Filter requirements.\n",
    "order = 4\n",
    "fs = 200.0       # sample rate, Hz\n",
    "cutoff = 2  # desired cutoff frequency of the filter, Hz\n",
    "\n",
    "xtrain_filtered = butter_lowpass_filter(xtrain, cutoff, fs, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerativeClassifierHMM(hmm=GaussianHMM(algorithm='viterbi', covariance_type='diag', covars_prior=0.01,\n",
       "      covars_weight=1, init_params='stmc', means_prior=0, means_weight=0,\n",
       "      min_covar=0.001, n_components=1, n_iter=10, params='stmc',\n",
       "      random_state=1234, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
       "      verbose=False))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm = GaussianHMM(n_iter=10, random_state=seed)\n",
    "hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "hmm_classifier.fit(xtrain_filtered[train_index], \n",
    "                   label_enc.transform(ytrain[train_index]),\n",
    "                   np.tile(15, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Validation Accuracy', 0.96887966804979253)\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy', (y_val_pred == ytrain[test_index]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[train_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Accuracy', 0.96409714889123543)\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy', (y_train_pred == ytrain[train_index]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross validation on number of states, cutoff frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "order = 4\n",
    "fs = 200.0  \n",
    "\n",
    "hmm = GaussianHMM(n_iter=10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: k = 10; cut_freq = 2 ---> 0.941175230565\n",
      "Validation Accuracy: k = 10; cut_freq = 3 ---> 0.974032995778\n"
     ]
    }
   ],
   "source": [
    "cv3_results = {}\n",
    "for k in range(10, 31):\n",
    "    for cutoff in [2, 3, 4, 5, 6, 7, 8, 9, 10]:  # desired cutoff frequency of the filter, Hz\n",
    "        xtrain_filtered = butter_lowpass_filter(xtrain, cutoff, fs, order)\n",
    "        val_acc = 0.0\n",
    "        tra_acc = 0.0\n",
    "        for train_index, test_index in kf.split(xtrain, ytrain):\n",
    "            hmm_classifier = GenerativeClassifierHMM(hmm)\n",
    "\n",
    "            hmm_classifier.fit(xtrain_filtered[train_index], \n",
    "                       label_enc.transform(ytrain[train_index]),\n",
    "                       np.tile(k, 20))\n",
    "\n",
    "            y_val_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[test_index]))\n",
    "            val_acc += (y_val_pred == ytrain[test_index]).mean()\n",
    "            \n",
    "            y_train_pred = label_enc.inverse_transform(hmm_classifier.predict(xtrain_filtered[train_index]))\n",
    "            \n",
    "            tra_acc += (y_train_pred == ytrain[train_index]).mean()\n",
    "            \n",
    "        val_acc /= kf.get_n_splits()\n",
    "        print('Validation Accuracy: k = ' + str(k) + '; cut_freq = ' + str(cutoff) + ' ---> ' + str(val_acc))\n",
    "        \n",
    "        cv3_results[(k, cutoff)] = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Appendix Material\n",
    "* scaling and guassian filter (although guassian filter didn't work) https://www.researchgate.net/publication/4090432_Principal_Component_Analysis_for_Online_Handwritten_Character_Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
